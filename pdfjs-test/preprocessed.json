[
  {
    "title": "Abstract",
    "noOfSentences": 11,
    "text": "In this paper we present a methodology to analyze usersâ€™ con-cerns and perspectives about privacy at scale. We leverage NLPtechniques to process millions of mobile app reviews and extractprivacy concerns. Our methodology is composed of a binary clas-sifier that distinguishes between privacy and non-privacy relatedreviews. We use clustering to gather reviews that discuss similarprivacy concerns, and employ summarization metrics to extractrepresentative reviews to summarize each cluster. We apply ourmethods on 287M reviews for about 2M apps across the 29 cate-gories in Google Play to identify top privacy pain points in mobileapps. We identified approximately 440K privacy related reviews.We find that privacy related reviews occur in all 29 categories, withsome issues arising across numerous app categories and other issuesonly surfacing in a small set of app categories. We show empiricalevidence that confirms dominant privacy themes â€“ concerns aboutapps requesting unnecessary permissions, collection of personalinformation, frustration with privacy controls, tracking and the sell-ing of personal data. As far as we know, this is the first large scaleanalysis to confirm these findings based on hundreds of thousandsof user inputs. We also observe some unexpected findings suchas users warning each other not to install an app due to privacyissues, users uninstalling apps due to privacy reasons, as well aspositive reviews that reward developers for privacy friendly apps.Finally we discuss the implications of our method and findings fordevelopers and app stores.",
    "summaryText": ""
  },
  {
    "title": "KEYWORDS",
    "noOfSentences": 0,
    "text": "privacy, nlp, mobile apps, empirical",
    "summaryText": ""
  },
  {
    "title": "1 INTRODUCTION",
    "noOfSentences": 66,
    "text": "In app stores, such as Google Play and Appleâ€™s App Store, userscan write reviews to share their experience and opinions about theapps that they use. Reviews help other users to understand whetheror not the app might be of interest to them. These reviews are alsoa feedback channel to developers who can learn how to improvetheir apps. App reviews can be a challenge to analyze as they areknown to cover a broad range of topics, have widely varying quality(that is somewhat exacerbated by their unstructured form) [54];thus it can be difficult for developers to parse out separate issues.Moreover, some issues, such as privacy related feedback, may havelower volume than other issues (e.g., battery performance), and thusmay be less visible. However extracting privacy related feedbackis of particular importance as by now developers are well awarethat trust is heavily impacted by their privacy posture, and becauseprivacy legislation and regulation are on the rise.Unstructured app reviews provide a potentially rich source ofcontent to learn about usersâ€™ perspectives on privacy. User feedbackcan be mined at scale to extract product requirements [45,68], how-ever these methods do not focus on privacy. Traditional methodsfor gaining insights into user perspectives about privacy includeconducting qualitative studies and surveys. Qualitative studies havethe advantage of being in depth as they involve one-on-one inter-views, however they arelimited in scaleto include typically 20-30participants. Surveys, on the other hand, have the advantage beingstructured and repeatable, however are also limited in terms ofscale typically to a few thousands. By leveraging automation andadvanced Natural Language Processing (NLP) techniques, feedbackby millions of users can be analyzed rapidly to extract privacy per-spectives. This approach complements existing qualitative methodsas it obtains privacy concerns on a new scale, yet cannot follow upwith users for additional information. Our approach enables datadriven decisions to be made - such as priority ranking across multi-ple issues. We therefore answer the following research questions:â€¢Can mobile app reviews be automatically analyzed at scaleto identify privacy related ones?â€¢Can the identified privacy reviews be used to understandusersâ€™ privacy concerns? How do they compare with con-cerns inferred from other qualitative methodologies?â€¢How can such a methodology be leveraged to inform theecosystem, and more specifically mobile app developers?This paper presents two main contributions. The first is a method-ology to analyze userâ€™s privacy concerns with mobile apps via NLPtechniquesat scale. The methodology includes a binary classifierto decide whether or not a particular review discusses a privacytopic, a mechanism to cluster reviews that discuss similar semantictopics, and a way to summarize the clusters by identifying reviewsthat are highly representative of all those belonging to a singlecluster. Our methodology opens the door to a rich set of subsequentresearch on user privacy perspectives (e.g., relative ranking of pri-vacy concerns, how these concerns evolve over time, why someprivacy issues occur predominantly within specific app categories,user perspectives on different elements of personal data, etc). It alsoenables tools for developers to receive feedback that can help themimprove the privacy of their products.Our second contribution uses the methodology to provideem-pirical evidence(a first large-scale analysis) that confirms domi-nant privacy themes that have been identified in qualitative stud-ies [3,25,36,47,79]. We apply our methods to 287M reviews, andreport on the wide variation of privacy reviews across app cate-gories, relative reviewing of specific app permissions and dominantprivacy themes. We also discuss the implications of our findingsfor developers and how they can inform the design of privacy toolsin app stores.There are many challenges to this problem space. First, thereis no labeled data. Second, the boundary of when a text is aboutprivacy or not is a fuzzy one; for example, many security and privacyexperts often do not agree on the amount of overlap between thesetwo areas [13,19]. Third, we need to be able to capture a broad rangeof privacy concerns, i.e., any topic that exists in established privacytaxonomies [4,67]. Fourth, to learn a broad range of contexts, weneed a classifier that can do more than memorization of keywords.Fifth, user reviews are well known to be variable in their writingstyle [54] which can lead to classification errors. We address allthese challenges herein.Our first contribution develops a multi-step method to addressthis problem. To generate our test, validation and training data weemploy a method that combines Expert-Hand-Labeling and Heuris-tic Supervision [61] (Challenge #1). In this bootstrapping processwe construct a set of regular expressions in consultation with lin-guistic experts, and based on our manual observations of how usersexpress privacy concerns in reviews (Challenge #2, #3). We nextdevelop a privacy classifier as an ensemble model that couples bothUSE [14] and BERT [20] deep learning models, which are trainedon user texts and are known to generalize well (Challenge #3, #4,#5). Our classifier achieves 98% precision and 87% recall. We useK-means clustering to group similar privacy reviews, and proposea new metric to determine a good value ofğ‘˜that produces compactsingle-issue clusters (Challenge #3). We summarize the topic clus-ters, by extracting representative reviews for the cluster. Overallthis yields a new method to understand user insights about privacythat complements traditional qualitative methods.The development of automated privacy text classifiers (for usertext) and the ability to separate privacy issues into distinct clustersin an unsupervised fashion, enables numerous types of large-scaleanalyses including (some of which are discussed in this paper) -ranking of issues, breadth of concerns across app types, unantici-pated and emerging issues, sentiment with which users approachspecific issues, user behavior (e.g. uninstalling an app), compar-isons across app types (e.g. childrenâ€™s versus regular apps), issuesper culture (as captured by language) and more. We discuss howsuch findings can inform developers about privacy shortcomingsof their apps, and more generally, how it could inform privacy bestpractices across app stores.Our second contribution is a large scale analysis using our method-ology on 287M reviews, coming from approximately 2M apps fromthe Play Store. We identify approximately 440K reviews that discussprivacy. We found that the privacy-related reviews exist in all ofthe 29 app categoriesin the Play store, and that these reviewscapture a breadth of privacy concerns and perspectives. We findmany reviews where users ask to know the purpose for a request tocollect personal information or permission gated data (e.g., location,contacts, camera, etc). We observed four somewhat unexpected find-ings: (1) users warn each other not to install an app due to privacyreasons; (2) users uninstall apps due to privacy concerns; (3) usersconcerns about the selling of personal data are largely confined toa small set of app categories; and (4) some users reward developerswhose apps are privacy friendly with privacy positive reviews.We apply our clustering and summarization to ten categoriesof apps and identify the large compact clusters within each cat-egory. From these, we identity five dominant privacy concernsacross multiple app categories: apps that appear to request unnec-essary permissions, collection of personal information, tracking,privacy controls and apps that may be selling personal data. Next,we present an overview of these dominant themes using our au-tomatically selected representative reviews that summarize eachcluster. While the dominant privacy themes that emerged from ouranalysis are generally well-known, we demonstrate the ability toautomate and scale this process. Our initial findings illustrate thepotential of such automated analysis. We conclude the paper witha discussion on implications for developers and app stores, limita-tions, and a summary of remaining challenges needed to furthermature this type of analysis.",
    "summaryText": ""
  },
  {
    "title": "2 RELATED WORK",
    "noOfSentences": 54,
    "text": "The primary use of NLP in the privacy space in the last few yearshas been to analyze privacy policies [28,31,40,49,53,56,60,63,65,66,80]. Numerous efforts have focused on identifying incon-sistencies between an applicationâ€™s source code and its privacypolicy [40,53,65,66,80] or itâ€™s description [28,56,60]. Other usesinclude developing privacy chat bots [32] and automated question-answer systems based on deep-learning techniques [31,63] to helpusers understand data practices. In [49], the authors explore the useof NLP to help users avoid inadvertently sharing personally identi-fying information in social media, emails, and text messages. In [57]the authors used NLP to analyze app description texts, within alarger mechanism to determine when two apps are similar; thiswas used to inform developers when they may be requesting anunnecessary permission. None of these privacy-oriented studiesapplied NLP to analyze the text in user reviews.User reviews have been analyzed using NLP, but not in the con-text of privacy. Pagano and Maalej [54] conducted an empiricalanalysis of iOS app reviews and found that reviews are not easy toautomatically analyze given their unstructured forms. Some effortshave built classifiers to identify informative app reviews [15] orreviews useful for app maintenance [55]; others identified incon-sistencies between user reviews and ratings, as well as performedtopic analysis of negative reviews to shed light on why users dislikea given app [26] and unsupervised topic discovery composed ofsemantically similar user comments based on bidirectional NLPalgorithms [43]; work has also been done to automatically matchbug reports with related app reviews [44]. In the RequirementsEngineering space there have been large bodies of work that fo-cused on feature/requirements extraction from user reviews and/orapplied a sentiment analysis to find out how users like a certainfeature [30,45,68]. [29] developed a review summarization frame-work to categorize app reviews into categories (e.g., bug reportsand feature requests), and also extracts aspects and their sentiments(e.g., interface is good/poor). Tian et.al. have shown that includingcrowd sourced review information in app update notifications wasmore effective at alerting users of invasive or malicious app updates,especially for less trustworthy apps [72].The prior research that is closest to ours is [8,51,52]. Besmeret al. [8] have trained a logistic regression model to detect privacyapp reviews, and have shown that privacy reviews have lower starratings and more negative sentiment, but have higher engagement(more upvotes/downvotes). In both [51,52], the authors developeda SVM classifier to extract reviews from the Play Store on the twotopics of security and privacy. The key purpose of [52] was todetermine if app reviews discussing security and privacy lead tochanges in the app, and [51] focused on how actual app behaviorinfluences usersâ€™ security and privacy concerns. Using static codeanalysis, the authors in [52] were able to demonstrate that thepresence of security and privacy reviews are predictive of securityand privacy app updates in 60% of the cases they looked at. Thisis very encouraging, as it means developers do respond to issuesraised in these types of reviews. As part of that work, the authorshad to develop a security and privacy review classifier to extractthose reviews. The authors in [51] use dynamic analysis to showusers security and privacy concerns are justified in that the appsdo often exhibit troublesome behavior along the lines indicated inreviews.Our work differs from these efforts in several ways. First, ourfocus is on privacy, not security and privacy combined. (We ac-knowledge that security and privacy are interwoven, at times, es-pecially when security issues such as account hacking or passwordmanagement have privacy consequences). Second, their classifiersrely on a basic SVM/Logistic Regression models and follow a bag-of-words approach. We use state of the art deep learning based NLPmodels (e.g., USE and BERT) that offer multiple advantages. Thesetransformer models are trained on large corpuses of text (e.g., BERTis trained on Wikipedia) and thus these models have the potential togeneralize beyond the labeled examples. As explained more fully inSection 3.2.1, a bag-of-words approach is likely to miss context, andthat matters for identifying privacy-related reviews across a broadset of privacy topics. Third, we work with a larger dataset. We man-ually labeled 11K examples (compared to 2.4K in [8], 4K in [52] and6K in [51]). We ran our inference analysis on 287M reviews. Fourth,our method incorporates clustering and summarization whereasthese prior efforts only included classification. This is needed sinceour end goal - to provide a way to report users perspectives at scale- is different.There is an enormous body of usable security research on userperceptions towards mobile app privacy. Most quantitative workhave found that people are very protective of their personal in-formation when using apps [36,77], and actively engage with of-fered privacy controls to safeguard their information [10,39,73].User frustration due to apps requesting unnecessary permissionshas been well studied [23,38,39,64,73,75]. In fact, users wereoften surprised by the abilities of applications to collect data inthe background [35,71], and were concerned with possible risksassociated with permissions [24]. Special attention has been onstudying the privacy concerns arising due to usersâ€™ location beingtracked [3, 17, 22].These prior studies reveal similar privacy concerns that we ob-serve in our research. However, they are all done by surveying fewhundred participants or interviewing 30 or less participants. Ourmethodology scales up to large numbers of user reviews and tracksthese issues efficiently across all apps, and our findings show theprevalence of specific privacy concerns across app categories.",
    "summaryText": ""
  },
  {
    "title": "3 METHODOLOGY",
    "noOfSentences": 9,
    "text": "Figure 1 illustrates an overview of our approach. On the left areexamples of reviews, some of which are about privacy and one ofwhich is not (marked in yellow). Our first task is to extract thosereviews that are related to privacy. This is achieved by constructinga binary classifier to distinguish between the privacy and non-privacy reviews. In figure 1, the review about taking pictures hasbeen filtered out in the sample list of â€œprivacy reviewsâ€. Our secondtask is to identify the common fine-grained privacy themes withinthe privacy-relevant app reviews. For this we leverage K-Meansclustering;â€˜kâ€™is a parameter that needs to be chosen upfront, and wepropose a custom metric to choose the best value forâ€˜kâ€™. We use theTensorFlow [1] machine learning tool for training a binary classifierand clustering the privacy-related app reviews. In the followingsections, we describe how we generate test, validation and trainingdata; present the design of our privacy classifier; and describe ourmethod for clustering and summarizing privacy-related reviews.",
    "summaryText": ""
  },
  {
    "title": "3.1 Dataset Curation",
    "noOfSentences": 44,
    "text": "In order to develop test, validation, training and inference datasets,we collected approximately 580M Play reviews in English publishedon the Play store between April 2014 and Feb 2020. Our dataset wasanonymized in terms of app names, as each review was only labeledby its app category; however we were given an aggregated appcount of 2M. We started with zero labeled data. As per the manymethods for generating labeled data, as outlined well in [61], weuse Expert-Hand-Labeling (by subject matter experts) for our testand validation datasets to ensure these are of the highest qualitysince they are used to evaluate the performance of our machinelearning models. Prior work [52] hints that privacy related reviewsmay constitute less than 1% of all reviews, hence we were facingan extremely imbalanced dataset. To bootstrap this procedure andgenerate candidate reviews that are likely to be about privacy we didthe following. We relied on two well known privacy taxonomies [4,67] to set the framing for our initial definition and scope of privacyissues. We curated an initial seed list of n-grams inspired by thesetaxonomies. Our manual labeling team consisted of the authorsFigure 1: Method pipeline.and three linguists. We filtered reviews based on this initial seedand conducted a preliminary manual evaluation. We looked to seethe types of words and expressions that users employ to expressprivacy concerns.There were two observations in this bootstrapping phase. First,filtering using 1-gram and 2-gram words can result in many falsepositives; for example, the single word â€œtrustâ€ can be used in â€œdonâ€™ttrust your teammatesâ€, which is not a statement about privacy.Thus, we decided to only use n-grams withğ‘›>=3for furtherfiltering. Second, we converted our seed list of n-grams to regularexpressions and expanded the set of expressions based on thismanual exploration. Regex patterns allow us to succinctly capturegrammatical variants of typical privacy statements. For every regexpattern, we checked to see if it occurred in at least 100 reviewsthat were privacy-related. Our regexes captured approximately 200n-grams. We acknowledge that our list may not be complete andmight have missed privacy issues or phrases used to discuss privacy.We next selectedâˆ¼11K app reviews for manual labeling. Toensure we ended up with enough labeled privacy examples, weselected 60% of the 11K reviews because they matched against ourregex patterns, and the other 40% were ensured to not match any ofthe regexes. Each of the 11K reviews were manually examined andcross-labeled by three raters and a label (privacy or not privacy) wasassigned. For the vast majority of reviews, all three labelers agreed.When this was not the case, discussion ensued until an agreementwas reached. Among the 11,371 manually labeled reviews (groundtruth),6688were labeled asprivacyand4683werenot privacy.The validation dataset used to evaluate the performance of the MLmodels after each training epoch was created by extracting250privacyand250 not privacyreviews from this set. The remain-ing 10,871 reviews were treated as the test set for comparing theperformance of different privacy classifiers.Because each of our regexes (capturing 200+ n-grams) was veri-fied â€“ by checking for its appearance in at least 100 privacy-relatedreviews, and also based on our manual labeling exercise â€“ we as-sume our regex list effectively constitutes a set of good qualityheuristics. We use these to generate our training data, as per themethod ofHeuristic-Supervisionas in [61]. We used roughly halfof the reviews (âˆ¼290M) to generate training samples. From this setwe identified 250K app reviews that matched our privacy regexpatterns and labeled them as privacy-related reviews; we then ran-domly sampled another 250K reviews that did not contain any ofour privacy regexes and labeled them as not-privacy. Our privacyregex patterns could only identify 0.08% of reviews as being relatedto privacy. Finally, we used the second half of our collected reviews,namely 287M reviews, as our inference dataset. We performed clas-sification, clustering and summarization on this set to understandusersâ€™ top privacy concerns. The sizes of our training, validationand test datasets are shows in Table 1.Table 1: Size of Data SetsEthical Considerations:Our institution approved the use ofthis dataset because Play reviews are already public. In compliancewith ethical training guidelines in our institution, we ensured thatusersâ€™ privacy were respected. We thus carried out the following.First, all researchers have been trained in ethical user research priorto this study. Second, the dataset was preprocessed to remove useridentifiers and app names before the researchers were given access.The only accompanying metadata beyond the review text, was theappâ€™s category name and the publish timestamp. Third, access tothis version of the dataset is limited to the authors of this paper.",
    "summaryText": ""
  },
  {
    "title": "3.2 Privacy Classifier",
    "noOfSentences": 94,
    "text": "The simplest approach to extract privacy related reviews might bevia a keyword list. However, it is non-trivial for multiple reasonsto curate a comprehensive list of keywords. In addition to thefalse positive issue discussed in Section 3.1, the same word couldmean different things depending on the context. For example, theoccurrence of â€œinvadingâ€ in a war game app review likely does notrefer to a privacy concern, whereas it might in a review about aparental control app with a location tracking feature (e.g. â€œthis appis invading my privacyâ€). Moreover, there could also be instanceswhere a review does not contain privacy keywords but, based onits context, still be related to privacy. For instance, â€œdonâ€™t want myfriend accessing my emailâ€ does not contain any privacy specifickeywords but is a privacy concern based on the context.Traditional approaches to analyze text rely on the bag-of-words [33]representation or use word embeddings, such as Word2Vec [50]and GloVe [58], to encode text. Such systems do not encode infor-mation about the word sequence, and therefore cannot differentiatebetween reviews containing the same words in different order. Forexample, â€œdelete cookies and website historyâ€ and â€œmy article onhistory of cookies got deleted from websiteâ€ use same words buthave different meanings. In addition to handling context, and wordsequence, we also need to be able to correctly process reviews thatare frequently unstructured, contain highly variable writing styles,grammatical errors and misspellings.The above limitations establish the need to process reviews by in-corporating robust NL Understanding components. In this work, weuse state-of-the-art pre-trained natural language models, BERT [20]and Universal Sentence Encoder (USE) [14], to efficiently encodeuser reviews into an abstract representation. These models are bet-ter known to capture contexts. We provide here a brief backgroundon the BERT and USE language models.BERT: BidirectionalEncoderRepresentations fromTransformers(BERT) [20] is a language representation model based on the Trans-former architecture that has been trained on 3.3 billion word corpus.The model was trained on two tasks: â€œmasked language modellingâ€,where the aim is to predict the masked-out words of the input textusing the information present in the surrounding words, and â€œnextsentence predictionâ€, where the model predicts the next sentencegiven the first sentence as the input. The large pre-trained BERTneural network model has had great success in NLU tasks, such astext summarization [41], question-answering [42], and has beenshown to deliver impressive performance on downstream taskseven when working with small training datasets [69]. Hence, wefine-tune the pre-trained BERT model on our training set to createa binary privacy classifier.USE: UniversalSentenceEncoder is another deep neural net-work based model, that uses encoders (Transformer-based [74] orDeep-Averaging-Network based [34]) to learn meaningful sentencerepresentations [14]. The model is trained on data from varioussources, such as Wikipedia, discussion forums, web questions andanswers, etc.; and has shown great performance in detecting fakenews spreaders on Twitter [46] and in learning cross-lingual textrepresentations [16]. Unlike BERT, we do not fine-tune the USEmodel, instead use the readily available pre-trained USE TF-Hubmodule(which provides text embedding representations directly)and build a two-layer feed-forward neural network on top for cre-ating our privacy classifier.3.2.1Models.We propose the following four model variants ascandidates for our privacy classifier.(Vanilla) BERT: We take the pre-trained BERT model, and addone additional network layer after the last layer for binary classifi-cation. We use a [CLS] token (start-of-sentence) to represent eachreview in its entirety. Thus our additional layer simply transformsthe embeddings learnt for the [CLS] token to the two classes, i.e.,privacy and not privacy. We fine-tune the BERT model on our train-ing data for three epochs, and choose the best epoch model basedon the performance on the validation dataset.Sentiment-aware BERT (BERT-SST): From a preliminary anal-ysis, we found that the privacy review texts generally have a neg-ative sentiment. We use this information to further strengthenour BERT-based privacy classifier, by first fine-tuning BERT forsentiment classification task. Using similar model architecture men-tioned above for (Vanilla) BERT, we first train the model on a text-sentiment dataset: Stanford Sentiment Treebank, that containstext and its binary (positive or negative) sentiment label. We thenfine-tune this model on our training data to create the privacyclassifier. We simply map the privacy class to the negative senti-ment class, and not-privacy to the positive one. We fine-tune thesentiment-aware BERT model for three epochs and choose the bestepoch model based on the performance on the validation set.USE: We extract a new512-dimensional embedding representa-tion for each review by passing the reviews through the pre-trainedUSE model that is based on Deep Averaging Network encoder. Wethen pass the embedding through a feed-forward neural networkwith2layers and512hidden units each. The output of the2layeris then mapped to the two classes, i.e., privacy and not-privacy. Themodel is trained with the objective to maximize the probability ofthe correct class, and thus we use cross-entropy loss to optimizethe network. We train the model for 20 epochs and chose the bestmodel based on performance on the validation set.Ensemble Model: In our experimentation, we noticed inconsis-tent label assignments by each of the three privacy classifiers above.(This is understandable as the underlying BERT and USE models arepre-trained on different datasets with distinct characteristics.) Tobetter understand the scenarios where USE and BERT models weremaking different decisions (privacy or not-privacy), we manuallyexamined about 1000+ reviews that had different labels from thetwo models. Below is an example of a statement that USE classifiedasprivacywhereas BERT labled itnot-privacy.â€œyou can record call automatically, record anonymous calls, recordimportant calls ... you can choose the phone numbers in the phone-book or recording call automatic. the list of recorded files will bestored and streamlined for you in the phone call recorder.â€The following is an example of a review that BERT predicted asprivacy whereas USE did not.â€œnot having tamil channels and sports channels which is the wayof looting the trust from the customer serviceâ€The first example is ambiguous as making anonymous phonecalls could be perceived as being related to privacy. However, thisreview mainly lists app features. The second example mentionstrust however this isnâ€™t a privacy issue but instead perhaps one offeeling excluded due to a language not supported in the app. Wewouldnâ€™t consider either of these to be about privacy.To reduce such ambiguities and improve our confidence in theprivacy review identification, our Ensemble model considers a re-view to be about privacy, if and only if, all three classifiers (USE,BERT and BERT-SST) labeled it as privacy. Note that this makesour model conservative (i.e., we will underestimate the numberof privacy reviews) since we are choosing to focus on precisionrather than recall. For our purposes of broadly understanding usersâ€™privacy concerns with mobile apps, we prefer to have less noise inour clusters. We acknowledge that a developer using such a methodmay opt for high recall to be sure not to miss any particular issue.3.2.2Performance of Models.We evaluate our four privacy classi-fier models (see Table 2) on the test dataset. High recall numbersindicate that the model is able to correctly identify most of theprivacy-related reviews, and a high precision indicates that themodel rarely labels a not-privacy review as being related to privacy.From the table, we see that the USE model has a high recall and aTable 2: Performance of Models testedrelatively low precision, meaning that it has more false positives(not-privacy reviews labeled as privacy). On the other hand, boththe BERT based models have a higher precision but low recall com-pared to USE. The BERT-SST model has higher accuracy than BERT,but looking at the F1 scores, vanilla BERT model performed betterthan BERT-SST. As expected the Ensemble model has the highestprecision; it also obtains the highest AUC value, and a good F1score. We use thus the Ensemble model in the remainder of thiswork for our classification task.We did a qualitative analysis to see if our ensemble classifier wasable to generalize its learning beyond the terms and expressions inour regex patterns. First we checked to see if our classifier learnedany concepts not included in our regex patterns. For example, wedid not include any terms related to â€œanonymityâ€ or â€œanonymousâ€in our regexes, however we did find a number of reviews thatmention â€œI like the anonymity ...â€. Our model likely learned that thewords related to â€œanonymousâ€ are often associated with privacybecause of the following. The following real review -â€œi donâ€™t wanta personalized profile full of surveillance. anonymous access is apreferredâ€- could have been flagged as privacy because of the word\"surveillance\" (that was in our regexes). Since this review alsocontains the word \"anonymous\", the classifier learns to associatethis with the privacy label (given enough similar examples). Second,we compared the fraction of privacy reviews that our regexes alonematch (0.08% in the 290M reviews used to generate our training data,Section 3.1), with those extracted by our classifier, namely (0.15%from our 287M reviews test data). This shows that our classifierdoes generalize beyond the terms and expressions in the regexes asit identifies roughly twice the amount of content as our regexes.",
    "summaryText": ""
  },
  {
    "title": "3.3 Clustering and Summarization",
    "noOfSentences": 87,
    "text": "The next step in our pipeline is Clustering and Summarization of theprivacy related reviews to tease out the different privacy concernsusers describe. We know from prior work as well as our curatedset of n-grams, there are a multitude of things users might writeabout, such as personal data collection, privacy controls, locationtracking, a feeling of being spied on, third party data sharing, newprivacy features, consents, etc. We refer to these asprivacy themes.Since app reviews do not have fine-grained labels for such privacythemes, we use unsupervised learning (specifically clustering) toidentify the common privacy issues. We use K-means clustering asour approach to clustering because it is simple and broadly used,and leave exploration of other clustering solutions as future work.We apply K-means to the set of privacy reviews per app category(Games, Parenting, Tools, etc). The motivation for studying cat-egories independently is that the number of reviews across appcategories was highly variable (ref Table 3). Analyzing all the re-views together would have not highlighted usersâ€™ concerns in appTable 3: Number / proportion of privacy reviews per categorycategories with lower numbers of reviews. Analyzing the categoriesindependently helped us identify issues which may be prominentin one category but not in others (e.g., tracking and selling datawas not a key concern in the Games category).Once these clusters are determined, we summarize the topicdiscussed in a cluster by selecting some highly representative re-views for each cluster. After independently reviewing the top rep-resentative reviews and labelling the clusters in each category, weperformed a second round of annotation where we cross-checkedthe cluster labels across categories and mapped clusters discussingthe same concerns to the broader topic.K-Means clustering: We use the 512-dimensional USE embed-ding generated for each review to perform clustering. We use em-beddings derived from our USE-based model instead of BERT-basedmodels as it lead to a higher AUC score (refer Table 2). The clus-tering is performed within an app category, andCosine Distanceis used as the distance metric. Like any other clustering task, thechallenge here is:how to determine a goodğ‘˜valuefor the number oftarget clusters, without knowing ahead of time how many distinctthemes users may be writing about. There exist various metrics inthe literature to chooseğ‘˜, such as the Silhouette Score[62], DunnIndex[21], CH-Score[12], etc. These metrics primarily reward wellseparated and compact clusters, which is also our goal. However,these metrics implicitly assume the following: 1) a sample belongsto only one cluster (or only belongs to one privacy theme); and 2)all samples belong to some cluster, i.e., no sample is considered tobe an exception or outlier. For privacy related Play reviews, theseassumptions do not always hold. Consider this example:â€œ1 star for forcing people to create an account. no itâ€™s not necessaryfor the user. itâ€™s apparently necessary for [APP_NAME_REDACTED].permissions they demand arenâ€™t necessary and invasive. this appis more like spyware. clearly [APP_NAME_REDACTED] is makingmoney off of peopleâ€™s personal and private information. i donâ€™trecommend this product. at minimum, if youâ€™re going to use thisapp, give them fake information. however they wonâ€™t even let youuse the app if you donâ€™t give them location tracking information.undoubtedly, theyâ€™ll be another company in the news soon forexploiting customer privacy.â€This example review touches on multiple privacy themes, includingunnecessary permissions, spyware, location tracking, and generalprivacy exploitation. There is potential that each of these fine-grained topics produce its own cluster when running K-Means; sothis example review would be hard to place within one cluster as itmight naturally be on the border of multiple clusters. Therefore, weaim to limit the influence of such reviews on the cluster formationby optimizing for the creation of well-formed clusters â€“ clustersthat predominantly discuss a single privacy issue. The next exampleillustrates the second assumption:â€œi wish it worked better for pregnancy milestones, bump pics, etc.all the reminders assume your kid is born regardless of entering thedue date. also, the pictures take forever to load. i uploaded multiplepics for one day and my sister thought it was just 1 pic because ofthe slow load time. COMPANY_NAME_REDACTED is faster and ican make a private group so my pics arenâ€™t super public.â€In this example, the user seems to be primarily expressing dissatis-faction about the reminder mechanisms and upload speeds. Theyalso mention that they would like a new privacy feature, namelythe ability to create private groups. The request for a private groupoption does make this a legitimate review about privacy. However,we did not find similar requests in other reviews in parenting apps,and thus this review is an outlier. We aim to limit the influence ofsuch reviews in the clusters produced.Given this, we have the following goals. Firstly, we aim for wellseparated clusters. Secondly, we aim to have a high number ofclusters that have limited mixed-concern reviews and to minimizethe number of outliers. To achieve this, we propose asummarizationmetricthat will reward a value ofğ‘˜based on these criteria.To address this first goal we use the following construct. Letğ‘†denote the center of clusterğ‘–. We defineğ‘‘ğ‘–ğ‘ ğ‘¡as:ğ‘‘ğ‘–ğ‘ ğ‘¡=min{ğ›¿(ğ‘†, ğ‘†)âˆ€{ğ‘–, ğ‘—}}(1)whereğ‘–â‰ ğ‘—andğ‘–, ğ‘—âˆˆ [0, ğ‘˜âˆ’1].ğ›¿(ğ‘†, ğ‘†)is the cosine distancebetween two cluster centers.ğ‘‘ğ‘–ğ‘ ğ‘¡represents the minimum pairwisedistance among all pairs ofğ‘˜cluster centers. Maximizingğ‘‘ğ‘–ğ‘ ğ‘¡ensures that the cluster centers be far apart in the vector space.We iterate through multiple values ofğ‘˜and select the one thatmaximizes this metric. This formula varies compared to existingmetrics, where an average of inter-cluster distance is generallytaken into account. Here we ensure that the minimum inter-clusterdistance is the highest for the chosenğ‘˜value.To address our goals of limiting the influence of mixed-concernreviews and outliers, we want intuitively to â€œignoreâ€ reviews thatareloosely associatedwith a cluster, as well as the mixed-concernreviews as these are likely to be â€œborderlineâ€ between multipleFigure 2: Silhouette Scores using K-Means for k=5clusters. We aim to count the reviews within a cluster that areclosely related, and refer to them asupvotes(defined below). Weuse thesilhouette scoreto do this. Recall that the silhouette scoreğ‘ (ğ‘–)formulates how close each point is to its cluster center and howfar it is from the nearest neighboring cluster, namelyğ‘ (ğ‘–)=ğ‘(ğ‘–) âˆ’ğ‘(ğ‘–)max(ğ‘(ğ‘–), ğ‘(ğ‘–))whereğ‘(ğ‘–)is the lowest average distance betweenğ‘–-th point andany cluster of which it is not a member, andğ‘(ğ‘–)is the averagedistance betweenğ‘–and all the other points of the same cluster. Asan example, we run K-Means withğ‘˜=5on reviews identified asprivacy in theParentingcategory and compute the silhouette scoresfor each review, as shown in Figure 2. A negative silhouette scoreindicates that the review is assigned to a wrong cluster, as it iscloser to the neighboring cluster. The dotted red line in Figure 2represents the average silhouette score across all the reviews in thecategory. A low silhouette score (close to zero) indicates that thereview is very close to the cluster boundary (data points/reviewswhich are on LHS of the red line, but still positive), and a highsilhouette score indicates the review is closer to its own clustercenter (data points/reviews on the RHS of the red line). We refer toreviews with silhouette score higher than the average silhouettescore asupvotesfor a given theme in a cluster. From Figure 2 wesee that it could be useful to retain clusters 0, 1, and 4 that havea larger amount of upvotes and ignore clusters 2 and 3. Clusters2 and 3 are likely to be poor quality since most of the silhouettescores are negative or below the average silhouette score. Hencein this illustrative example, we would aim to have 3 final clustersand simply not capture the ignored clusters which are unlikely tocontribute to top issues.We consider clusters to be compact when they have a high num-ber of upvotes, and a low number of mixed-concern reviews in agiven cluster. Therefore, we can rephrase our goal as aiming for akthat results in high number ofcompactclusters where a compactcluster is defined as one in which at least 30% of the samples areupvotes. We refer to the number of compact clusters identified asğ‘€, for a givenk.We combine the above two principles, and define a newSum-marization Metricas follows:Summarization Metric=(ğ‘‘ğ‘–ğ‘ ğ‘¡âˆ—ğ‘€)(2)We iterate throughğ‘˜=2, ...,10and choseğ‘˜for which the summa-rization score is highest, as we want to increase bothğ‘‘ğ‘–ğ‘ ğ‘¡(distancebetween cluster centers) andğ‘€(number of compact clusters). Weuseğ‘€to denote the final number of compact clusters identified forthe chosenğ‘˜. We considerğ‘˜between 2 and 10 because in our initialexploration using our methodology, we focus on dominant privacyconcerns. However using a largerğ‘˜would enable an analyst to lookthrough the long-tail of privacy concerns.We may end up with tens of thousands of reviews in a singlecluster, thus it is important to summarize them in a way that cap-tures the primary concern. We do this by selecting a few specificreviews that can be considered as representative of the reviews inthe entire cluster. By summarizing this way, we capture the clustertopicin the usersâ€™ own words. We rank reviews within a clusteraccording to their silhouette scores and use the top ten reviewswith the highest scores as the representative reviews. We carefullyanalyzed these cluster representatives manually across all clustersand verified that these reviews indeed illustrate the main topic ineach cluster. In the next sections, we select quotes/reviews thatcame from different categories to show the breadth of the issuesacross app categories.",
    "summaryText": ""
  },
  {
    "title": "3.4 Limitations",
    "noOfSentences": 9,
    "text": "We used K-means as our first approach to clustering because it issimple and broadly used. However, our clusters are quite unevenin size, and thus more sophisticated approaches (such as AffinityClustering [7]) could yield improved performance.This kind of work is inherently hard because the definition ofprivacy is not exact. We relied on previously accepted privacytaxonomies, three professional linguists, and our own experiencereading huge numbers of privacy reviews. A clearer sense of thedistinction (or accepted overlaps) between security, censorship, ha-rassment and privacy could take the form of agreed upon guidelinesby a community of domain experts.Although user perspectives within a cluster are automaticallysummarized by the representative sentences (ranked by silhouettescores), there is still a manual step in assigning thematic labels toeach cluster (i.e privacy controls, selling data, etc.) - that we didby reading the top 20 reviews per cluster and finding a label basedupon our interpretation of those reviews. NLP methods for topiclabeling [48, 78] could automate this step - although their efficacyin the privacy domain needs to be evaluated.",
    "summaryText": ""
  },
  {
    "title": "4 PRIVACY THEMES",
    "noOfSentences": 34,
    "text": "In this section, we present the top privacy themes that are associatedwith different app categories. To do this, we run our clusteringanalysis on the previously extracted privacy reviews. Due to spacelimitations, we present findings from 10 Play Store app categoriesinstead of all 29 categories. We selected categories that were eitherlarge or ones where we expected privacy concerns might arise.Recall that our clustering iterates throughğ‘˜=2...10 and picks thebestğ‘˜(number of clusters) according to our summarization metric.For each of our 10 app categories, we found thatğ‘˜varied from 6 to9. We looked at the compact clusters across these 10 categories, andidentified 5 themes that were dominant across multiple categories.Within each cluster considered, we ranked the reviews in thoseclusters by silhouette score. Recall that the top ranked reviewsessentially represent the topic of the cluster as they are central tothe cluster. For each of our selected clusters, we manually read thetop 20 most representative reviews, and assigned a short thematiclabel to the cluster for ease of presentation and for summarization.For example, we assigned the thematic labeltoo many permis-sionsto clusters whose representative reviews frequently mentionthat an app requests more permissions than what seems needed.These reviews are referring to the Android permissions such aslocation, contacts, microphone, camera, etc. This theme was presentacross all app categories, with location permission being the highestoccurring sub-theme. In other large clusters, many reviews com-ment on the collection ofpersonal information. Such reviewstypically refer to information such as address, email, profile info, etc.Other themes we identified includeprivacy controls, trackingandselling data. The cluster of reviews assigned theprivacy-controlstheme captures reviews that either discuss the existingprivacy controls of an app, or ask for a new privacy feature to beadded to the app. Table 4 shows our themes and app categories;there is an asterisk in each table entry if that theme appears as thetop-5 issue for the respective app category. A blank cell means thatthis topic did not surface as one of ourğ‘€compact clusters for agiven app category (there might still be reviews on this topic, butthey are not large enough to generate a compact cluster).While these privacy themes have been identified in qualitativestudies before [3,25,36,47,79],our analysis is the first largescale work to confirm these findings empirically. Moreover,we are able to quantify the volume per theme thereby enabling usto rank these issues, and we are able to show which issues occuracross multiple app categories (e.g. permissions) and which occurin only a few categories (e.g. selling data).Given the scope of each of the themes, our purpose here is not todive into details individually, but instead to summarize succinctlyhow users express these pain points. The included examples belowcome from our top ten representative reviews per cluster (topic), perapp category. Because these examples are central to clusters of manythousands of similar reviews, they summarize the views of largegroups of users. Our clusters ranged from a few thousand up to 20Kin size. Among these representative reviews, we selected examplesfrom different categories to illustrate that privacy concerns arerarely category specific. A more detailed look into each specificprivacy theme is left as future work.",
    "summaryText": ""
  },
  {
    "title": "4.1 Concern 1: Too Many Permissions",
    "noOfSentences": 21,
    "text": "The largest cluster, in each of our 10 categories, contains reviewsthat make comments about the app asking for too many permis-sions. This is expected as permissions are the gateway to accessingpersonal information, and prior work has pointed to excessive per-missions being a major concern [23,38,39,64,73,75]. Examplesinclude:Table 4: Privacy Topic themes occurring in App Categories(1)â€œi really enjoyed this app until i realized it had access to all ofmy photos , media and storage ! why the heck would a simplesudoku app need that ! ? iâ€™m now more careful at checking thepermissions before i install anything . i promptly deleted thisapp and installed a similar sudoku app that doesnâ€™t requiresuch ridiculous permissions and itâ€™s just as goodâ€(Games)(2)â€œwhy does this app require access to my contacts? purpose? itake my privacy very seriously. no one should install this appif you value your privacy.â€(Maps & Navigation)(3)â€œgood app, but i hate it when apps request permissions it hasno business for (in this case, access to my contacts), no thanksâ€(Auto & Vehicles)The first two reviews focus on whether the data collected is reallyneeded for the functionality of the app. In example 1, the disconnectbetween the permissions asked for and the userâ€™s perception of theapp functionality caused this user to uninstall the app, and switchto a more privacy-friendly app. In example 2, the user is asking forthe purpose of collecting contacts (presumably because it is notclear from the context). The importance of sharing purpose withusers has been established in the academic literature [6,36,73], andeven in Androidâ€™s best practice guidelines; it is interesting to nowsee users effectively demanding that. It is important for developersto learn when users have such concerns, as they can be mitigatedby providing explanations [40,70] to address the â€œpurposeâ€ typequestions. In example 3, users state they feel that data collectionfrom some permission requests is â€œunacceptableâ€ or a â€œriskâ€.Figure 3: Privacy reviews that discuss permissionsWe also examined which reviews mention a specific permission,such as microphone, location, etc. To do this we used simple key-word matching such asphone, call log, contact, microphone, location,sms, and checked if the wordpermissionappeared somewhere in thereview. Figure 3 shows the number of privacy reviews that mentioneach permission group as well as the percentage of reviews theyrepresent. We see that the location permission is the most discussedpermission, appearing in over 40,000 reviews (13% of the privacyreviews). Contacts is the second most discussed permission.",
    "summaryText": ""
  },
  {
    "title": "4.2 Concern 2: Too Much Personal Information",
    "noOfSentences": 33,
    "text": "Reviews in which users complain that too much personally identi-fiable information (PII) is being collected is the second dominantprivacy concern, and occurred in eight of our ten categories exam-ined herein. While prior work has shown that users are concernedabout too much personal information being collected, both in thecontext of mobile apps [36,77] and generally [2,11], unlike oursthey do not rely on experiences users have using their own devicesin the wild. Our analysis additionally shows that users warn oth-ers not to download an app explicitly because of PII collection, asshown in the following two examples:(1)â€œwhen it wonâ€™t let you play the game unless you agree to letit use your information , it ainâ€™t worth playing . if you wantprivacy , donâ€™t download itâ€(Games)(2)â€œdont do this!!! youre only giving the app your personal info!what you look like, your finger prints, everything! i downloadedthis app just to warn everyone not to give this app permission toanything on your device! ... this is dangerous!â€(Entertainment)Other users indicate their suspicion i.e., there is no good reasonfor the data collection. These suspicions, which are in essencerequests for data collection justification, reflects the same issue wesaw in the reviews abouttoo-many-permissionsâ€“ applied todifferent data items. In the first two examples below, the users areclearly quite annoyed. The user in the 3rd example implies thatthey might have uninstalled the app because of this reason.(1)â€œwhy the hell you need access to everything, you are serviceprovider or intruder in privacy.â€(Auto & Vehicles)(2)â€œhorrific registration process; requiring personal informationirrelevant to the applicationâ€™s purpose.â€(Maps and Navigation)(3)â€œwhy does it need my device ids and the phone numbers of mycallers. who i communicate with is none of their business. thisapp appears to be collecting more info than it requires to offeritâ€™s services ... they have no respect for my privacy, the app wasuseful.â€(Health and Fitness)Yet other users express themselves with terms relating to theftor harassment, as the following two examples convey. Clearly trustis impacted if users interpret an appâ€™s behavior this way.(1)â€œstarted stealing personal information. phone numbers andmessages are being read by this application.â€(Maps and Navi-gation)(2)â€œtoo much personal info: why do you need so much personalinfo you creeps? are you the offenders looking for prey? jeezâ€(Parenting)",
    "summaryText": ""
  },
  {
    "title": "4.3 Other Dominant Privacy Concerns",
    "noOfSentences": 68,
    "text": "Privacy Controls.Comments about privacy controls were a com-mon issue across 5 app categories, namely in Communication, Par-enting, Photography, Tools and Games. The fraction of reviewsdiscussing this theme ranged from 24% for Communication appsto 6% for Games. The reviews about privacy controls show an in-teresting breadth, from explicitly requesting privacy controls to beeasier (example 1), to frustration with privacy controls (example2), to requesting new features (e.g. private chat in example 3). Inexample 2, the user is trying to hide some photos within app, yetdoes not appear to be able to do so successfully. To address the userfrustration with using the offered privacy controls, personalizedprivacy assistants have been proposed [39, 73].(1)â€œdonâ€™t share my pics on public domain without my permission,when people search for specific location to visit pics appear, somake privacy settings easy.â€(Photography)(2)â€œlock the pics you dont wanna see then they just get copiedright back to the gallery doesnt hide anything.â€(Tools)(3)â€œi love this game ! it is so addictive and fun ! ... the only thing iwould want to change add is private chat.â€(Games)Tracking.We saw tracking as a dominant concern in 5 app cate-gories, namely Communications, Entertainment, Parenting, Pho-tography and Medical apps. The fraction of privacy reviews thatdiscuss tracking ranged from a minimum of 10% in Entertainmentapps up to 38% in Parenting apps. Privacy concerns arising dueto usersâ€™ location being tracked have been well studied [3,17,22],but our analysis shows that location isnâ€™t the only attribute peopleare concerned about being tracked. Purchase history, contacts, andother personal information are also important. As the examplesbelow show, users sentiment on this topic can range from annoyedto angry. Since our methods can extract reviews on this issue, itpermits future work on sentiment analysis and perhaps a deeperexploration into which types of data are more sensitive. (Note, thethird example mentions private information as well as spying andthus provides an example of reviews that fall on the boundary oftwo themes (one of the challenges in Section 3.3)).(1)â€œwarning: this app is spyware and will track all your locationinfo and purchases.â€(Entertainment)(2)â€œspyware, data miner. will not connect until you grant accessto your phone location and data. contacts and location arepersonal information. cameras donâ€™t need this to function. youlie to public!â€(Photography)(3)â€œi agree with protecting your child. but when your a teen likeme you feel like you cant breath without constantly beingwatched. itâ€™s like being stalked by your own family, and as ifyou canâ€™t trust them. thereâ€™s a boundary between protectionand privacy. and this is stepping over the line.â€(Parenting).Selling Data.Usersâ€™ have previously expressed concerns with ser-vice providers selling their data [18,76]. We show here that upsetdue to the perception of personal data being â€œsoldâ€ to third partiesalso appears in mobile apps. Unexpectedly though, we found thisto be dominant in only 3 app categories - that of Communications,Medical and Entertainment apps. In the Communications apps, weobserved approximately 20% of the privacy reviews mentioningselling user data, making this a significant privacy issue for Com-munication apps. The examples below hint that this issue appearsto make users feel undermined, as the comments are quite cynical.We also see that users imagine their data being sold off to a varietyof recipients, including to companies, the NSA, and spammers.(1)â€œi would highly recommend staying away from this cash grabof an app and move to an app that actually cares about the datait collects about you. i feel like this company, and applicationare only used to track your data and information and sellingit off to the highest bidder, sad thing is, they make you pay forit, so youâ€™re essentially paying to get your information stolen.â€(Medical)(2)â€œthey are selling patientâ€™s personal data to corporates. once uuse [COMPANY-NAME-REDACTED], u will start getting mailsand call from labs for medical tests. beware.â€(Medical)(3)â€œwhat i do not understand is why [COMPANY-NAME-REDACTED]had to make another way that they could sell our private in-formation to third parties like the NSA.â€(Communications)Privacy Positive Reviews.During the above exercise processingthe top privacy pain points, we found - to our surprise - that whilemost privacy reviews are negative, we do see some privacy pos-itive reviews. In these reviews, users mention that they like theprivacy controls (examples 1 and 2) or that they are grateful the appis not collecting unnecessary information (example 3). The verypresence of such reviews indicates that developers can be rewardedby privacy-friendly design. Examples include:(1)â€œthis is one of the best photo sharing apps out there. no needto share your childrenâ€™s whole lives on social media and messaround with tons of privacy settings. you invite who you wantto your album and can share privately with your partner orthe whole family.â€(Parenting)(2)â€œgood way of keeping photos private.â€(Photography)(3)â€œit was great fun playing this. i like that they donâ€™t want accessto your private information unlike other apps.â€(Games)While these five themes were dominant concerns, there weresmaller review clusters that touched upon other topics, such asâ€œads related to personal informationâ€ and â€œsafety concerns due topersonal information leakageâ€. The diversity of privacy issues isbroad thus making it challenging to provide examples of all topics,due to lack of space.Overall we found many of these reviews to be fairly privacysavvy - many questioned the purpose of data collected and de-manded justification, while others suggested specific privacy con-trols they would like to see added. Our quotes from representativereviews show that it is not uncommon for users to warn otherothers to stay away from an app for privacy related reasons. Thedominant issue users are concerned about is the collection of toomuch personal data, be it from app permissions or PII. We foundthis to be true across nearly all app categories. We found reviewsof users who uninstall apps due to privacy related reasons; thisis important to know as we suspect that developers are not com-pletely aware when this happens. We also found privacy positivereviews and this indicates that developers can be publicly rewardedfor privacy friendly behavior.",
    "summaryText": ""
  },
  {
    "title": "5 IMPLICATIONS",
    "noOfSentences": 24,
    "text": "We illustrate how our method enables developers to improve theprivacy of their products, and how app stores can leverage ourfindings to design better privacy tools and best practices.Organizing and understanding user feedback for a particularapp in a much more meaningful and actionable way.Today,the Play Developer Console [27] surfaces all privacy related userreviews under a theme called â€œPrivacyâ€. This makes it difficultto effectively understand user feedback and identify specific painpoints. Instead, embedding our methodology into such developerfeedback channels would allow app authors to address nuancedprivacy concerns directly. For instance, concerns on:â€¢Too many permissions: if a developer sees many users com-plaining that they do not see the purpose for a permissionrequest, then a guideline would suggest to provide a mean-ingful explanation or to remove the permission entirely.â€¢Selling Data: if a set of reviews shows much concern aboutselling personal data and the app does not engage in suchbehavior, then a developer would be advised to provide aneducational intervention to clarify this misunderstanding.â€¢Privacy controls: if reviews identify missing features or ca-pabilities then these can be translated into product require-ments or bug fixes [43, 44].Guiding the design of new tools for developers to providetransparency on their data-collection practices, such as theApple Privacy Nutrition Labels [5], and its upcoming equivalentin the Play Store [59]. Understanding user concerns at scale andin depth via our methodology could inform the design of suchtransparency initiatives.â€¢Analyzing user concerns abouttracking, selling data, and toomuch personal informationcould shed light on what typeof labels are useful and how they should be designed. Thetwo label systems above are a good step forward as they en-courage developers to be upfront about their collection andsharing practices. Such labels could increase user trust andhelp them differentiate between the privacy stance of com-peting apps. Although proper use of such labels is turningout to be challenging [37].â€¢In the long term, our methodology will enable refining suchlabels and tailor them to the specific concerns of users. Itwill also allow monitoring of emerging privacy concerns.Mechanism for providing insight into users perceptions ofprivacy. Beyond looking at top issues, one could evaluate howprivacy issues vary by country, by language, by time, or compareprivacy issues between childrenâ€™s apps versus regular apps. Run-ning sentiment analysis on such privacy text data sets, using toolssuch as Stanfordâ€™s NLTK [9], would enable large-scale comparisonof sentiment across privacy issues and help developers and appstores prioritize what to fix first. These insights would be veryvaluable, especially for small scale apps that do not have resourcesto analyse their reviews another way.Nudging developers towards better privacy practices. For in-stance, today the Play Developer Console nudges developers to re-move permissions that are not used by apps in its peer groups [57].By using our methodology, these nudges can be expanded to theother privacy concerns identified in this paper. For example, to mit-igate tracking concerns, developers could be informed of maliciousad libraries as opposed to those that are much more privacy safe.",
    "summaryText": ""
  },
  {
    "title": "6 CONCLUSION",
    "noOfSentences": 5,
    "text": "Understanding privacy trends is key to address systemic concernsacross ecosystems and populations. To date, large-scale evidenceof where the main privacy concerns lie for app users has been lack-ing. This paper is the first to provide a methodology that enablesautomated analysis and succinct summarization of privacy feed-back, on a large scale. Our methodology can act as a mechanism forkey ecosystem stakeholders to be responsive to evolving societalconcerns regarding privacy. As new technologies come to the foreand as the risks of large-scale data harvesting become more appar-ent, this is a stepping stone towards systematic understanding ofprivacy concerns at scale.",
    "summaryText": ""
  },
  { "title": "References", "noOfSentences": 0, "text": "", "summaryText": "" }
]
